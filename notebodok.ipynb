{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e04f2c12",
   "metadata": {},
   "source": [
    "# <font size=\"65\">Benchmarking Robust Classification Under Data Imbalance</font>\n",
    "\n",
    "Trabalho realizado por:\n",
    "\n",
    "* Afonso Coelho (FCUP_IACD:202305085)  \n",
    "* Diogo Amaral (FCUP_IACD:202305187)  \n",
    "* Miguel Carvalho (FCUP_IACD:202305229)  \n",
    "\n",
    "<div style=\"padding: 10px;padding-left:5%\">\n",
    "<img src=\"fotos_md/Cienciasporto.png\" style=\"float:left; height:75px;width:200px\">\n",
    "<img src=\"fotos_md/Feuporto.png\" style=\"float:left ; height:75px; padding-left:20px;width:200px\">\n",
    "</div>\n",
    "\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "******\n",
    "\n",
    "## 0. Projeto\n",
    "\n",
    "Este projeto insere-se na unidade curricular de *Machine Learning I (CC2008)* e tem como objetivo aprofundar a compreensão teórica e prática de algoritmos de classificação supervisionada, aplicando-os em cenários com características desafiantes nos dados.  \n",
    "\n",
    "Em particular, a nossa abordagem foca-se na **classificação binária com dados desbalanceados**, uma situação comum em domínios médicos, financeiros e industriais, onde uma das classes é significativamente mais rara do que a outra.  \n",
    "\n",
    "Partimos de uma implementação base de um algoritmo clássico de classificação, que modificamos para lidar melhor com este tipo de desbalanceamento. O desempenho do modelo original e do modelo modificado será comparado em vários conjuntos de dados de benchmark, com o objetivo de validar empiricamente a eficácia das alterações propostas.  \n",
    "\n",
    "O projeto decorre em duas fases:\n",
    "- **Fase 1**: Análise e avaliação da versão original do algoritmo.\n",
    "- **Fase 2**: Proposta e teste de uma versão modificada, mais robusta ao desbalanceamento.\n",
    "\n",
    "Todos os algoritmos são implementados **de raiz**, sem recurso a bibliotecas de alto nível como `scikit-learn`, respeitando os requisitos do enunciado.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa558d",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Importante correr com env feito pelo ficheiro [requirements](requirements.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88309271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from mla.metrics.metrics import mean_squared_error\n",
    "from mla.neuralnet import NeuralNet\n",
    "from mla.neuralnet.constraints import MaxNorm\n",
    "from mla.neuralnet.layers import Activation, Dense, Dropout\n",
    "from mla.neuralnet.optimizers import Adadelta, Adam,RMSprop\n",
    "from mla.neuralnet.parameters import Parameters\n",
    "from mla.neuralnet.regularizers import L2\n",
    "from mla.utils import one_hot\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import importlib\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupx_y(dataset_path):\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Supondo que o dataset tenha colunas 'features' e 'target'\n",
    "    X = data.iloc[:, :-1].values  # Todas as colunas menos a última como features\n",
    "    y = data.iloc[:, -1].values   # Última coluna como target\n",
    "\n",
    "    values, counts = np.unique(y, return_counts=True)\n",
    "    majority_ratio = counts.max() / counts.sum()\n",
    "\n",
    "    if len(values) != 2:\n",
    "        raise ValueError(\"A coluna target não é binária\")\n",
    "\n",
    "    if majority_ratio <= 0.6:\n",
    "        raise ValueError(\"Nenhuma classe é majoritária (>60%)\")\n",
    "\n",
    "    majority_class = values[np.argmax(counts)]\n",
    "    minority_class = values[np.argmin(counts)]\n",
    "\n",
    "    # Re-encode: majority -> 0, minority -> 1\n",
    "    y = np.where(y == majority_class, 0, 1)\n",
    "\n",
    "    # Count the occurrences of 0's and 1's in the target column\n",
    "    count_zeros = np.sum(y == 0)\n",
    "    count_ones = np.sum(y == 1)\n",
    "    print(f\"Count of 0's: {count_zeros}, Count of 1's: {count_ones}\")\n",
    "\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    \n",
    "    y = one_hot(y)\n",
    "    return X, y, count_zeros, count_ones\n",
    "    \n",
    "\n",
    "import math\n",
    "\n",
    "def calcular_batch_size(n_total, n_minor, k=3, batch_min=32, batch_max=512):\n",
    "    \"\"\"\n",
    "    Calcula um batch size adequado com base no desequilíbrio entre classes.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - n_total: total de amostras\n",
    "    - n_minor: número de amostras da classe minoritária\n",
    "    - k: fator multiplicador de segurança (padrão = 3)\n",
    "    - batch_min: tamanho mínimo aceitável (padrão = 32)\n",
    "    - batch_max: tamanho máximo permitido (padrão = 512)\n",
    "\n",
    "    Retorna:\n",
    "    - batch_size ajustado\n",
    "    \"\"\"\n",
    "    if n_minor == 0:\n",
    "        raise ValueError(\"Número de amostras minoritárias não pode ser zero.\")\n",
    "\n",
    "    imbalance_ratio = (n_total - n_minor) / n_minor\n",
    "    estimated_batch = math.ceil(k * imbalance_ratio)\n",
    "\n",
    "    # Respeita os limites definidos\n",
    "    batch_size = max(batch_min, estimated_batch)\n",
    "    batch_size = min(batch_size, n_total, batch_max)\n",
    "\n",
    "    return batch_size\n",
    "\n",
    "def custom_stratified_split(X, y_onehot, test_size=0.15, minority_ratio_test=0.10, random_state=1111):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import numpy as np\n",
    "\n",
    "    # Converter one-hot para binário\n",
    "    y = np.argmax(y_onehot, axis=1)\n",
    "\n",
    "    # Separar classes\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    if len(unique) != 2:\n",
    "        raise ValueError(\"Target y deve ser binário\")\n",
    "    minority_class = unique[np.argmin(counts)]\n",
    "    majority_class = unique[np.argmax(counts)]\n",
    "\n",
    "    # Separar X e y\n",
    "    X_min, y_min = X[y == minority_class], y[y == minority_class]\n",
    "    X_maj, y_maj = X[y == majority_class], y[y == majority_class]\n",
    "\n",
    "    # Split da classe minoritária: 90% treino, 10% teste\n",
    "    X_min_train, X_min_test, y_min_train, y_min_test = train_test_split(\n",
    "        X_min, y_min, test_size=minority_ratio_test, stratify=y_min, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Calcular total de amostras para o conjunto de teste\n",
    "    total_test_size = int(test_size * len(y))\n",
    "    n_remaining = total_test_size - len(y_min_test)\n",
    "    if n_remaining < 0:\n",
    "        raise ValueError(\"minority_ratio_test demasiado alto para o tamanho total de teste\")\n",
    "\n",
    "    # Split da classe majoritária para preencher o resto do teste\n",
    "    X_maj_train, X_maj_test, y_maj_train, y_maj_test = train_test_split(\n",
    "        X_maj, y_maj, test_size=n_remaining, stratify=y_maj, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Combinar splits\n",
    "    X_train = np.concatenate([X_min_train, X_maj_train])\n",
    "    y_train = np.concatenate([y_min_train, y_maj_train])\n",
    "    X_test = np.concatenate([X_min_test, X_maj_test])\n",
    "    y_test = np.concatenate([y_min_test, y_maj_test])\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=random_state)\n",
    "    X_test, y_test = shuffle(X_test, y_test, random_state=random_state)\n",
    "\n",
    "    # Re-transformar para one-hot\n",
    "    y_train_onehot = np.eye(2)[y_train]\n",
    "    y_test_onehot = np.eye(2)[y_test]\n",
    "\n",
    "    return X_train, X_test, y_train_onehot, y_test_onehot\n",
    "def classification(dataset_path,antes,filename):\n",
    "\n",
    "    # Carregar o dataset\n",
    "    X, y, count_zeros, count_ones = setupx_y(dataset_path)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = custom_stratified_split(X, y, test_size=0.15, minority_ratio_test=0.10,random_state=1111)\n",
    "    arr = np.asarray(y_train)\n",
    "\n",
    "    if arr.ndim > 1 and arr.shape[1] > 1:\n",
    "        # one-hot → labels 0/1\n",
    "        labels = np.argmax(arr, axis=1)\n",
    "    else:\n",
    "        labels = arr.flatten()\n",
    "\n",
    "    zeros = np.sum(labels == 0)\n",
    "    uns   = np.sum(labels == 1)\n",
    "\n",
    "    print(f\"Nº de zeros: {zeros}\")\n",
    "    print(f\"Nº de uns  : {uns}\")\n",
    "\n",
    "    batchsize= calcular_batch_size(X_train.shape[0],np.sum(y_train == 1), k=3, batch_min=8, batch_max=X_train.shape[0])\n",
    "    if antes==0:\n",
    "        model = NeuralNet(\n",
    "            layers=[\n",
    "                Dense(128, Parameters(init=\"uniform\", regularizers={\"W\": L2(1e-3)})),\n",
    "                Activation(\"relu\"),\n",
    "                Dropout(0.5),\n",
    "                Dense(64, Parameters(init=\"uniform\", regularizers={\"W\": L2(1e-3)})),\n",
    "                Activation(\"relu\"),\n",
    "                Dropout(0.3),\n",
    "                Dense(2),\n",
    "                Activation(\"sigmoid\"),\n",
    "            ],\n",
    "            filename=filename,\n",
    "            loss=\"focal_loss\",\n",
    "            testarerros=True,\n",
    "            zeros=zeros,\n",
    "            uns=uns,\n",
    "            count_ones=count_ones,\n",
    "            count_zeros=count_zeros,\n",
    "            optimizer=RMSprop(),\n",
    "            metric=\"f1score\",\n",
    "            batch_size=batchsize,\n",
    "            equilibrar_batches=True,\n",
    "            max_epochs=30,\n",
    "            shuffle=True,\n",
    "            l2=True,\n",
    "            dropout=True\n",
    "        )\n",
    "    elif antes==1:\n",
    "        model = NeuralNet(\n",
    "            layers=[\n",
    "                Dense(128),\n",
    "                Activation(\"relu\"),\n",
    "                Dense(64),\n",
    "                Activation(\"relu\"),\n",
    "                Dense(2),\n",
    "                Activation(\"sigmoid\"),\n",
    "            ],\n",
    "            filename=filename,\n",
    "            loss=\"binary_crossentropy\",\n",
    "            testarerros=True,\n",
    "            zeros=zeros,\n",
    "            uns=uns,\n",
    "            count_ones=count_ones,\n",
    "            count_zeros=count_zeros,\n",
    "            optimizer=RMSprop(),\n",
    "            metric=\"f1score\",\n",
    "            equilibrar_batches=True,\n",
    "            max_epochs=30,\n",
    "            shuffle=True,\n",
    "            l2=False,\n",
    "            dropout=False\n",
    "        )\n",
    "    elif antes==2:\n",
    "        model = NeuralNet(\n",
    "            layers=[\n",
    "                Dense(128, Parameters(init=\"uniform\", regularizers={\"W\": L2(1e-3)})),\n",
    "                Activation(\"relu\"),\n",
    "                Dropout(0.5),\n",
    "                Dense(64, Parameters(init=\"uniform\", regularizers={\"W\": L2(1e-3)})),\n",
    "                Activation(\"relu\"),\n",
    "                Dropout(0.3),\n",
    "                Dense(2),\n",
    "                Activation(\"sigmoid\"),\n",
    "            ],\n",
    "            filename=filename,\n",
    "            loss=\"automatic_weighted_binary_crossentropy\",\n",
    "            testarerros=True,\n",
    "            zeros=zeros,\n",
    "            uns=uns,\n",
    "            count_ones=count_ones,\n",
    "            count_zeros=count_zeros,\n",
    "            optimizer=RMSprop(),\n",
    "            metric=\"f1score\",\n",
    "            batch_size=batchsize,\n",
    "            equilibrar_batches=True,\n",
    "            max_epochs=30,\n",
    "            shuffle=True,\n",
    "            l2=True,\n",
    "            dropout=True\n",
    "        )\n",
    "    model.fit(X_train, y_train,X_test,y_test)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b1fb1",
   "metadata": {},
   "source": [
    "# Criar modelos e treinar os modelos\n",
    "\n",
    "> Nota: Demora cerca de 15 min "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()  \n",
    "dataset_dir = os.path.join(cwd, \"fixed_datasets\") \n",
    "modelos=[]\n",
    "for filename in os.listdir(dataset_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        dataset_path = os.path.join(dataset_dir, filename)\n",
    "        for i in range(3):\n",
    "            modelos.append(classification(dataset_path,i,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sanitize_data(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert numpy objects to native Python types for JSON serialization.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return _sanitize_data(obj.tolist())\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [_sanitize_data(item) for item in obj]\n",
    "    elif isinstance(obj, np.generic):  # numpy scalar\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def adicionar_modelo_ao_dataset(model, dataset=None):\n",
    "    \"\"\"\n",
    "    Adiciona os dados de um modelo ao dataset, convertendo one-hot para labels binárias.\n",
    "    \"\"\"\n",
    "    # Converter one-hot para labels binárias (0 ou 1)\n",
    "    y_train = getattr(model, 'y', None)\n",
    "    y_test = getattr(model, 'Y_test', None)\n",
    "    \n",
    "    # Se os dados estão em one-hot, extrair as labels\n",
    "    if y_train is not None and len(y_train.shape) > 1:\n",
    "        y_train = np.argmax(y_train, axis=1)  # Assume one-hot na última dimensão\n",
    "    if y_test is not None and len(y_test.shape) > 1:\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Preparar dados sanitizados\n",
    "    nova_linha = {\n",
    "        'ficheiro': model.filename,\n",
    "        'epocas': int(getattr(model, 'max_epochs', 0)),\n",
    "        'loss_nome': getattr(model, 'loss_name', None),\n",
    "        'l2': float(getattr(model, 'l2', 0.0)),\n",
    "        'dropout': float(getattr(model, 'dropout', 0.0)),\n",
    "        'zeros': int(getattr(model, 'zeros', 0)),\n",
    "        'uns': int(getattr(model, 'uns', 0)),\n",
    "        'train_out': json.dumps(_sanitize_data(y_train)),  # Labels binárias\n",
    "        'test_out': json.dumps(_sanitize_data(y_test)),    # Labels binárias\n",
    "        'probs': json.dumps(_sanitize_data(getattr(model, 'metric_list', None))),\n",
    "        'loss_list': json.dumps(_sanitize_data(getattr(model, 'loss_list', None))),\n",
    "    }\n",
    "\n",
    "    # Criar ou atualizar o DataFrame\n",
    "    linha_df = pd.DataFrame([nova_linha])\n",
    "    if dataset is None:\n",
    "        dataset = linha_df\n",
    "    else:\n",
    "        dataset = pd.concat([dataset, linha_df], ignore_index=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77495084",
   "metadata": {},
   "source": [
    "# Gerar o dataset com os modelos Aprox 350MB\n",
    "\n",
    "> Nota: Demora cerca de 30sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95740651",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"resultados.csv\"\n",
    "\n",
    "for i, model in enumerate(modelos):\n",
    "    linha_df = adicionar_modelo_ao_dataset(model)\n",
    "    if i == 0:\n",
    "        # Cria o arquivo com cabeçalho\n",
    "        linha_df.to_csv(csv_file, index=False, mode='w', header=True)\n",
    "    else:\n",
    "        # Anexa sem cabeçalho\n",
    "        linha_df.to_csv(csv_file, index=False, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac4fb1c",
   "metadata": {},
   "source": [
    "# Plotar métricas \n",
    "**processar_modelos_subset**: Todas as métricas com True de cada ficheiro. Modo visual pode ser:\n",
    "* Cada - um gráfico para cada loss e métrica.\n",
    "* Juntos - um gráfico com todas as losses num so gráfico.\n",
    "* Todos - Os gráficos de cada e Todos ``` Atenção gera +500 gráficos não é aconselhado para visualização```.\n",
    "\n",
    "**plotar_gráficos**: Um gráfico do racio das classes * nr de amostras pelas métricas com True  de todos os ficheiros num so gráfico. Mode visual pode ser:\n",
    "* Cada - um gráfico para cada loss e metrica.\n",
    "* Juntos - um gráfico com todas as losses num so gráfico.\n",
    "* Todos - Os gráficos de cada e Todos.\n",
    "\n",
    "**gerar_tabelas_percentuais_métricas**: Gerar 3 tabelas com cada metrica e o nr de amostras que surpassam a percentagem da coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27aad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fazerdataset import processar_modelos_subset , plotar_graficos, gerar_tabelas_percentuais_metricas\n",
    "df = pd.read_csv(\"resultados.csv\")\n",
    "processar_modelos_subset(df,roc_curve=True,\n",
    "                              f1_score=True,\n",
    "                              gmean=True,\n",
    "                              confusion_matrix=True,\n",
    "                              prec_recall_curve=True,\n",
    "                              modo_visual='juntos')\n",
    "plotar_graficos(\"resultados.csv\",roc_vs_ratio=True,\n",
    "                                prec_rec_vs_ratio=True,\n",
    "                                gmean_vs_ratio=True,\n",
    "                                modo_ratio='weighted',\n",
    "                                modo_visual='todos')\n",
    "gerar_tabelas_percentuais_metricas(df, limiares=(0 ,0.2,0.5, 0.7, 0.8, 0.9),min_test_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teste_hipoteses import comparar_losses_metricas\n",
    "df = pd.read_csv(\"resultados.csv\")\n",
    "comparar_losses_metricas(df, alpha=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Borgwarner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
