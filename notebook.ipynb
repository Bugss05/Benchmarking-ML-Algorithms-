{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298f2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Total parameters: 34178\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 96\u001b[0m\n\u001b[0;32m     91\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression mse\u001b[39m\u001b[38;5;124m\"\u001b[39m, mean_squared_error(y_test, predictions\u001b[38;5;241m.\u001b[39mflatten()))\n\u001b[1;32m---> 96\u001b[0m classification()\n",
      "Cell \u001b[1;32mIn[14], line 52\u001b[0m, in \u001b[0;36mclassification\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1111\u001b[39m)\n\u001b[0;32m     36\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralNet(\n\u001b[0;32m     37\u001b[0m     layers\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     38\u001b[0m         Dense(\u001b[38;5;241m256\u001b[39m, Parameters(init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m, regularizers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m: L2(\u001b[38;5;241m0.05\u001b[39m)})),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,\n\u001b[0;32m     51\u001b[0m )\n\u001b[1;32m---> 52\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     53\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     54\u001b[0m auc_score \u001b[38;5;241m=\u001b[39m roc_auc_score(y_test[:, \u001b[38;5;241m0\u001b[39m], predictions[:, \u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Calculate AUC\u001b[39;00m\n",
      "File \u001b[1;32md:\\JuniorAI\\Benchmarking-ML-Algorithms-\\MLAlgorithms\\mla\\neuralnet\\nnet.py:83\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Pass neural network instance to an optimizer\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\JuniorAI\\Benchmarking-ML-Algorithms-\\MLAlgorithms\\mla\\neuralnet\\optimizers.py:25\u001b[0m, in \u001b[0;36mOptimizer.optimize\u001b[1;34m(self, network)\u001b[0m\n\u001b[0;32m     22\u001b[0m     network\u001b[38;5;241m.\u001b[39mshuffle_dataset()\n\u001b[0;32m     24\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 25\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_epoch(network)\n\u001b[0;32m     26\u001b[0m loss_history\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m network\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32md:\\JuniorAI\\Benchmarking-ML-Algorithms-\\MLAlgorithms\\mla\\neuralnet\\optimizers.py:51\u001b[0m, in \u001b[0;36mOptimizer.train_epoch\u001b[1;34m(self, network)\u001b[0m\n\u001b[0;32m     48\u001b[0m     batch \u001b[38;5;241m=\u001b[39m tqdm(batch, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(network\u001b[38;5;241m.\u001b[39mn_samples \u001b[38;5;241m/\u001b[39m network\u001b[38;5;241m.\u001b[39mbatch_size)))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m---> 51\u001b[0m     loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(network\u001b[38;5;241m.\u001b[39mupdate(X, y))\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(network)\n\u001b[0;32m     53\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[1;32md:\\JuniorAI\\Benchmarking-ML-Algorithms-\\MLAlgorithms\\mla\\neuralnet\\nnet.py:88\u001b[0m, in \u001b[0;36mNeuralNet.update\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfprop(X)\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_grad(y, y_pred)\n",
      "File \u001b[1;32md:\\JuniorAI\\Benchmarking-ML-Algorithms-\\MLAlgorithms\\mla\\neuralnet\\nnet.py:99\u001b[0m, in \u001b[0;36mNeuralNet.fprop\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward propagation.\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 99\u001b[0m     X \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward_pass(X)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[1;32md:\\JuniorAI\\Benchmarking-ML-Algorithms-\\MLAlgorithms\\mla\\neuralnet\\layers\\basic.py:73\u001b[0m, in \u001b[0;36mDense.forward_pass\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_pass\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_input \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight(X)\n",
      "File \u001b[1;32md:\\JuniorAI\\Benchmarking-ML-Algorithms-\\MLAlgorithms\\mla\\neuralnet\\layers\\basic.py:76\u001b[0m, in \u001b[0;36mDense.weight\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweight\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m---> 76\u001b[0m     W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m W \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\jogos\\anaconda\\envs\\Seguranca\\Lib\\site-packages\\autograd-1.7.0-py3.13.egg\\autograd\\tracer.py:48\u001b[0m, in \u001b[0;36mprimitive.<locals>.f_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_box(ans, trace, node)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from mla.metrics.metrics import mean_squared_error\n",
    "from mla.neuralnet import NeuralNet\n",
    "from mla.neuralnet.constraints import MaxNorm\n",
    "from mla.neuralnet.layers import Activation, Dense, Dropout\n",
    "from mla.neuralnet.optimizers import Adadelta, Adam\n",
    "from mla.neuralnet.parameters import Parameters\n",
    "from mla.neuralnet.regularizers import L2\n",
    "from mla.utils import one_hot\n",
    "import pandas as pd\n",
    "from mla.neuralnet.layers.recurrent import LSTM, RNN\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "def classification():\n",
    "    # Generate a random binary classification problem.\n",
    "    dataset_path = \"d:/JuniorAI/Benchmarking-ML-Algorithms-/MLAlgorithms/class_imbalance/class_imbalance/dataset_764_analcatdata_apnea3.csv\"\n",
    "    data = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Supondo que o dataset tenha colunas 'features' e 'target'\n",
    "    X = data.iloc[:, :-1].values  # Todas as colunas menos a última como features\n",
    "    y = data.iloc[:, -1].values   # Última coluna como target\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    rnn_layer = LSTM(128, return_sequences=False)\n",
    "    y = one_hot(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1111)\n",
    "\n",
    "    model = NeuralNet(\n",
    "        layers=[\n",
    "            Dense(256, Parameters(init=\"uniform\", regularizers={\"W\": L2(0.05)})),\n",
    "            Activation(\"relu\"),\n",
    "            Dropout(0.5),\n",
    "            Dense(128, Parameters(init=\"normal\", constraints={\"W\": MaxNorm()})),\n",
    "            Activation(\"relu\"),\n",
    "            Dense(2),\n",
    "            Activation(\"softmax\"),\n",
    "        ],\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=Adadelta(),\n",
    "        metric=\"accuracy\",\n",
    "        batch_size=64,\n",
    "        max_epochs=25,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    auc_score = roc_auc_score(y_test[:, 0], predictions[:, 0])  # Calculate AUC\n",
    "    print(\"Roc Score Area under the curve\", auc_score)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test[:, 0], predictions[:, 0])\n",
    "\n",
    "    # Plot FPR vs TPR\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=\"ROC Curve (AUC = {:.2f})\".format(auc_score))  # Use AUC score here\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "    plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "    plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def regression():\n",
    "    # Generate a random regression problem\n",
    "    X, y = make_regression(n_samples=5000, n_features=25, n_informative=25, n_targets=1, random_state=100, noise=0.05)\n",
    "    y *= 0.01\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)\n",
    "\n",
    "    model = NeuralNet(\n",
    "        layers=[\n",
    "            Dense(64, Parameters(init=\"normal\")),\n",
    "            Activation(\"linear\"),\n",
    "            Dense(32, Parameters(init=\"normal\")),\n",
    "            Activation(\"linear\"),\n",
    "            Dense(1),\n",
    "        ],\n",
    "        loss=\"mse\",\n",
    "        optimizer=Adam(),\n",
    "        metric=\"mse\",\n",
    "        batch_size=256,\n",
    "        max_epochs=15,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"regression mse\", mean_squared_error(y_test, predictions.flatten()))\n",
    "\n",
    "\n",
    "\n",
    "classification()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Seguranca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
